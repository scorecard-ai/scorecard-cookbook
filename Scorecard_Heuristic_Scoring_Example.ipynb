{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scorecard-ai/scorecard-cookbook/blob/main/Scorecard_Heuristic_Scoring_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Scorecard Heuristic Scoring Example - Exact String Match\n",
        "\n",
        "## 🧙‍♂️ Instructions\n",
        "\n",
        "1. Create an account and [login to Scorecard](https://app.getscorecard.ai/). Copy your [API key](https://app.getscorecard.ai/api-key).\n",
        "1. Add your Scorecard and OpenAI API Keys below.\n",
        "1. Go to `Runtime` -> `Run all`. Enjoy!"
      ],
      "metadata": {
        "id": "LjecdsSRPPai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 👉 API Keys\n",
        "\n",
        "OPENAI_API_KEY = \"\" #@param { type: \"string\" }\n",
        "SCORECARD_API_KEY = \"\" #@param { type: \"string\" }"
      ],
      "metadata": {
        "id": "wAsJ8kJ6O1TI",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "xO5brg_tzz4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "#@markdown In order to keep the notebook working for all future users, we pin the dependency versions.\n",
        "\n",
        "!pip install scorecard-ai=='v1.0.0-beta0'\n",
        "!pip install openai==1.11.1"
      ],
      "metadata": {
        "id": "B_wcdhcPzyQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "\n",
        "from openai import OpenAI\n",
        "from scorecard.client import Scorecard\n"
      ],
      "metadata": {
        "id": "dod8Xp-5z5i7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build your LLM system\n",
        "\n",
        "Now, let's define your system (aka system-under-test)! For this demo, we'll set up an LLM call to generate the opening line of a story, where the user determines what the topic of the story will be."
      ],
      "metadata": {
        "id": "PQriXDnF0CVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define our multi-message prompt template\n",
        "\n",
        "PROMPT_TEMPLATE_1 = \"You are a helpful assistant.\" #@param { type:\"string\" }\n",
        "\n",
        "PROMPT_TEMPLATE_2 = \"Assist the user in crafting a story about {user_topic}.\" #@param { type:\"string\" }\n",
        "\n",
        "PROMPT_TEMPLATE_3 = \"I need a good opening line for my story. Please generate only the opening line.\" #@param { type:\"string\" }"
      ],
      "metadata": {
        "id": "-4PGlf0UQ2pV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iRiB7PWwzcu_"
      },
      "outputs": [],
      "source": [
        "#@title Call OpenAI to generate a story\n",
        "#@markdown Here we'll define an example of a multi-message prompt sent to OpenAI.\n",
        "\n",
        "def generate_story(user_topic: str) -> str:\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # or \"gpt-4\" depending on your access and requirements\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": PROMPT_TEMPLATE_1},\n",
        "        {\"role\": \"system\", \"content\": PROMPT_TEMPLATE_2.format(user_topic=user_topic)},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE_3}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate your system\n",
        "\n",
        "## Pre-req: Create Heuristic Metrics **[DO NOT SKIP]**\n",
        "\n",
        "First, using the [Scoring Lab](https://app.getscorecard.ai/scoring-lab) in the Scorecard application, create your metrics and scoring config.\n",
        "\n",
        "For this example,\n",
        "create a binary metric called **Exact String Match**, which compares the model output and ideal response to make sure they are an exact match. After that, create a Scoring Config that includes the newly created metric.\n",
        "\n",
        "Once you have created your Scoring Config, copy the ID and enter it below:"
      ],
      "metadata": {
        "id": "YCbMNYhl4IMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure Metrics\n",
        "HEURISTIC_METRIC_ID = 293  #@param { type: \"number\" }\n",
        "SCORING_CONFIG_ID = 187  #@param { type: \"number\" }"
      ],
      "metadata": {
        "id": "rWIaBWy_WvhV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Create a basic Testset\n",
        "#@markdown Here we'll create a basic Testset that gets stored in Scorecard.\n",
        "\n",
        "client = Scorecard(\n",
        "    api_key=SCORECARD_API_KEY\n",
        ")\n",
        "\n",
        "# Create a Testset\n",
        "testset = client.testset.create(\n",
        "    name=\"Story Opening Lines with Ideal\",\n",
        "    description=\"Demo of a testset created via Scorecard Python SDK\",\n",
        "    using_retrieval=False\n",
        ")\n",
        "\n",
        "# Add three testcases\n",
        "client.testcase.create(\n",
        "    testset_id=testset.id,\n",
        "    user_query=\"magical powers to control ice and snow\",\n",
        "    ideal=\"sample ideal response\",\n",
        ")\n",
        "client.testcase.create(\n",
        "    testset_id=testset.id,\n",
        "    user_query=\"a journey with a rugged iceman, his loyal reindeer, and a naive snowman\",\n",
        "    ideal=\"sample ideal response\",\n",
        ")\n",
        "client.testcase.create(\n",
        "    testset_id=testset.id,\n",
        "    user_query=\"the story of two royal sisters\",\n",
        "    ideal=\"sample ideal response\",\n",
        ")\n",
        "\n",
        "print(\"Visit the Scorecard app to view your Testset:\")\n",
        "print(f\"https://app.getscorecard.ai/view-dataset/{testset.id}\")"
      ],
      "metadata": {
        "id": "KqtQRU694Jft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Define heuristic scoring function\n",
        "#@markdown Do not hit the \"Run Scoring\" button to run scoring. Instead, implement the heuristic scoring function and kick off the scoring here in the SDK.\n",
        "\n",
        "def is_exact_string_match(model_response, ideal_response):\n",
        "    return model_response == ideal_response"
      ],
      "metadata": {
        "id": "TDVMDhAoWgL6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Execute the Testset and run heuristic scoring\n",
        "#@markdown Now we'll create a new Run to execute our LLM system above.\n",
        "\n",
        "from scorecard.types import RunStatus\n",
        "\n",
        "run = client.run.create(\n",
        "    testset_id=testset.id,\n",
        "    scoring_config_id=SCORING_CONFIG_ID,\n",
        ")\n",
        "client.run.update_status(run_id=run.id, status=\"running_execution\")\n",
        "\n",
        "for testcase in client.testset.get_testcases(testset_id=testset.id).results:\n",
        "    model_response = generate_story(user_topic=testcase.user_query)\n",
        "    testrecord = client.testrecord.create(run_id=run.id,\n",
        "                           testset_id=testset.id,\n",
        "                           testcase_id=testcase.id,\n",
        "                           user_query=testcase.user_query,\n",
        "                           response=model_response)\n",
        "    client.score.create(\n",
        "        run_id=run.id,\n",
        "        testrecord_id=testrecord.id,\n",
        "        metric_id=HEURISTIC_METRIC_ID,\n",
        "        int_score=None,\n",
        "        binary_score=is_exact_string_match(model_response, testcase.ideal),\n",
        "        reasoning=\"SDK generated heuristic score\",\n",
        "    )\n",
        "\n",
        "client.run.update_status(run_id=run.id, status=\"completed\")\n",
        "\n",
        "print(\"Visit the Scorecard app to view your Run:\")\n",
        "print(f\"https://app.getscorecard.ai/view-grades/{run.id}\")"
      ],
      "metadata": {
        "id": "JCc2EdyeRoIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}