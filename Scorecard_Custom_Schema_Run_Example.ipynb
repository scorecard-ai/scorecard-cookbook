{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scorecard-ai/scorecard-cookbook/blob/main/Scorecard_Custom_Schema_Run_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Scorecard Custom Schema Run Example\n",
        "## 🧙‍♂️ Instructions\n",
        "\n",
        "1. Create an account and [login to Scorecard](https://app.getscorecard.ai/). Copy your [API key](https://app.getscorecard.ai/settings).\n",
        "1. Add your Scorecard and OpenAI API Keys below.\n",
        "1. Go to `Runtime` -> `Run all`. Enjoy!"
      ],
      "metadata": {
        "id": "LjecdsSRPPai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 👉 API Keys\n",
        "\n",
        "OPENAI_API_KEY = \"\" #@param { type: \"string\" }\n",
        "SCORECARD_API_KEY = \"\" #@param { type: \"string\" }"
      ],
      "metadata": {
        "id": "wAsJ8kJ6O1TI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "xO5brg_tzz4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "#@markdown In order to keep the notebook working for all future users, we pin the dependency versions.\n",
        "\n",
        "!pip install scorecard-ai=='v1.0.0-beta0'\n",
        "!pip install openai==1.11.1"
      ],
      "metadata": {
        "id": "B_wcdhcPzyQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "\n",
        "from openai import OpenAI\n",
        "from scorecard.client import Scorecard\n"
      ],
      "metadata": {
        "id": "dod8Xp-5z5i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build your LLM system\n",
        "\n",
        "Now, let's define your system (aka system-under-test)! For this demo, we'll set up an LLM call to generate the opening line of a story, where the user determines what the topic of the story will be."
      ],
      "metadata": {
        "id": "PQriXDnF0CVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define our multi-message prompt template\n",
        "\n",
        "PROMPT_TEMPLATE_1 = \"You are a helpful assistant.\" #@param { type:\"string\" }\n",
        "\n",
        "PROMPT_TEMPLATE_2 = \"Assist the user in crafting a story about {user_query}.\" #@param { type:\"string\" }\n",
        "\n",
        "PROMPT_TEMPLATE_3 = \"I need a good opening line for my story containing the word '{keyword}', with a total characters of {max_chars} or less. Please generate only the opening line.\" #@param { type:\"string\" }"
      ],
      "metadata": {
        "id": "-4PGlf0UQ2pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRiB7PWwzcu_"
      },
      "outputs": [],
      "source": [
        "#@title Call OpenAI to generate a story\n",
        "#@markdown Here we'll define an example of a multi-message prompt sent to OpenAI.\n",
        "\n",
        "def generate_story(user_query: str, keyword: str, max_chars: int) -> str:\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # or \"gpt-4\" depending on your access and requirements\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": PROMPT_TEMPLATE_1},\n",
        "        {\"role\": \"system\", \"content\": PROMPT_TEMPLATE_2.format(user_query=user_query)},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE_3.format(keyword=keyword, max_chars=max_chars)}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate your system\n",
        "\n",
        "### Pre-req: Create Metrics\n",
        "\n",
        "First, using the Scorecard application, create your metrics and scoring config. For this example,\n",
        "we can use something simple like a Helpfulness metric that determines whether\n",
        "the generation adheres to the user's request.\n",
        "\n",
        "Once you have created your scoring config, copy the ID and enter it below:"
      ],
      "metadata": {
        "id": "YCbMNYhl4IMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure Metrics\n",
        "SCORING_CONFIG_ID = 1  #@param { type: \"number\" }"
      ],
      "metadata": {
        "id": "rWIaBWy_WvhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Create a Testset with custom variables\n",
        "#@markdown Here we'll create a basic Testset that gets stored in Scorecard.\n",
        "\n",
        "from scorecard.types import CustomSchema, CustomVariable\n",
        "import json\n",
        "\n",
        "client = Scorecard(\n",
        "    api_key=SCORECARD_API_KEY\n",
        ")\n",
        "\n",
        "# Create a Testset\n",
        "testset = client.testset.create(\n",
        "    name=\"Story Opening Lines\",\n",
        "    description=\"Demo of a testset created via Scorecard Python SDK\",\n",
        "    using_retrieval=False,\n",
        "    custom_schema=CustomSchema(\n",
        "        variables=[\n",
        "            CustomVariable(\n",
        "                name=\"inputs\",\n",
        "                description=\"Custom Parameters for this Testset.\",\n",
        "                role=\"input\",\n",
        "                data_type=\"json_object\",\n",
        "            ),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Add 2 testcases\n",
        "client.testcase.create(\n",
        "    testset_id=testset.id,\n",
        "    user_query=\"magical powers to control ice and snow\",\n",
        "    custom_inputs={\n",
        "        \"inputs\": json.dumps(\n",
        "            {\n",
        "                \"keyword\": \"ice\",\n",
        "                \"max_chars\": 100\n",
        "            }\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "client.testcase.create(\n",
        "    testset_id=testset.id,\n",
        "    user_query=\"a journey with a rugged iceman, his loyal reindeer, and a naive snowman\",\n",
        "    custom_inputs={\n",
        "        \"inputs\": json.dumps(\n",
        "            {\n",
        "                \"keyword\": \"reindeer\",\n",
        "                \"max_chars\": 200\n",
        "            }\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"Visit the Scorecard app to view your Testset:\")\n",
        "print(f\"https://app.getscorecard.ai/view-dataset/{testset.id}\")"
      ],
      "metadata": {
        "id": "KqtQRU694Jft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Run Tests\n",
        "#@markdown Now we'll create a new Run to execute our LLM system above.\n",
        "\n",
        "from scorecard.types import RunStatus\n",
        "\n",
        "run = client.run.create(testset_id=testset.id)\n",
        "client.run.update_status(run_id=run.id, status=\"running_execution\")\n",
        "\n",
        "for testcase in client.testset.get_testcases(testset_id=testset.id).results:\n",
        "  custom_variables = json.loads(testcase.custom_inputs[\"inputs\"])\n",
        "  model_response = generate_story(\n",
        "      user_query=testcase.user_query,\n",
        "      keyword=custom_variables[\"keyword\"],\n",
        "      max_chars=custom_variables[\"max_chars\"],\n",
        "  )\n",
        "\n",
        "  client.testrecord.create(run_id=run.id,\n",
        "                           testset_id=testset.id,\n",
        "                           testcase_id=testcase.id,\n",
        "                           user_query=testcase.user_query,\n",
        "                           custom_inputs=testcase.custom_inputs,\n",
        "                           response=model_response)\n",
        "\n",
        "client.run.update_status(run_id=run.id, status=\"awaiting_scoring\")\n",
        "\n",
        "print(\"Visit the Scorecard app to view your Run:\")\n",
        "print(f\"https://app.getscorecard.ai/view-grades/{run.id}\")"
      ],
      "metadata": {
        "id": "JCc2EdyeRoIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Kick off Scoring\n",
        "#@markdown Once your run above is finished executing, hit the \"Run Scoring\" button to run scoring. Once that's done, visit the Results page:\n",
        "\n",
        "print(\"Visit the Scorecard app to view your Results:\")\n",
        "print(f\"https://app.getscorecard.ai/view-grades/{run.id}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TDVMDhAoWgL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}